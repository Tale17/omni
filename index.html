<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Learning disentangled visual concepts to generate different concepts in a reference image or combine concepts from different images.">
  <meta name="keywords" content="Concept Disentangle, Diffusion, Image Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OmniPrism: Learning Disentangled Visual Concept for Image Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">OmniPrism: Learning Disentangled Visual Concept for Image Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/Tale17">Yangyang Li</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://daqingliu.github.io/">Daqing Liu</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://faculty.ustc.edu.cn/liuwu">Wu Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Allen He</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://xinchenliu.com">Xinchen Liu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=hxGs4ukAAAAJ">Yongdong Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://dblp.org/pid/61/9936">Guoqing Jin</a><sup>3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Science and Technology of China,</span>
            <span class="author-block"><sup>2</sup>JD Explore Academy, JD.com Inc.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Tale17/OmniPrism"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- 主图. -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/fig1.png"
       class="interpolation-image"
       alt="Interpolate start reference image."/>
      <h2 class="content has-text-centered">
        <span class="dnerf">We</span> propose OmniPrism, which arbitrarily disentangles and combines visual concepts. 
        (a) Disentangled visual concept generation. Given a reference image with multiple concepts, 
        our method can disentangle the desired concept guided by natural language such as content names 
        (red color words in prompts), “style” or “composition” (e.g., relation or structural features 
        like pose) while remaining faithful to prompts. (b) Multi-concept combination. Given two or more 
        reference images with the corresponding concept guidance, our approach can combine all desired 
        concepts in any combination without conflicts.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Creative visual concept generation often draws inspiration from specific concepts in a reference image to produce relevant outcomes.
            However, existing methods are typically constrained to single-aspect concept generation or are easily disrupted by irrelevant concepts in multi-aspect concept scenarios, leading to concept confusion and hindering creative generation. 
            To address this, we propose OmniPrism, a visual concept disentangling approach for creative image generation.
            Our method learns disentangled concept representations guided by natural language and trains a diffusion model to incorporate these concepts.
            We utilize the rich semantic space of a multimodal extractor to achieve concept disentanglement from given images and concept guidance.
            To disentangle concepts with different semantics, we construct a paired concept disentangled dataset (PCD-200K), where each pair shares the same concept such as content, style, and composition.
            We learn disentangled concept representations through our contrastive orthogonal disentangled (COD) training pipeline, which are then injected into additional diffusion cross-attention layers for generation.
            A set of block embeddings is designed to adapt each block's concept domain in the diffusion models.
            Extensive experiments demonstrate that our method can generate high-quality, concept-disentangled results with high fidelity to text prompts and desired concepts. Our codes, models, and datasets will be available upon acceptance.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-2">Method</h2>
        <img src="./static/images/method.png"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
        <div class="content has-text-justified">
          <p>
            (a) Given the reference image <code>I<sub>ref</sub></code>, target prompt <code>T<sub>tar</sub></code> and concept guidance <code>T<sub>cg</sub></code>, the concept extractor disentangles concept representations <code>f<sub>cpt</sub></code> by concatenating CLIP features <code>f<sub>cg</sub></code> of <code>T<sub>cg</sub></code> with a learnable query <code>q</code>, and feeds <code>f<sub>cpt</sub></code> into additional cross-attention layers in U-Net to generate target image <code>I<sub>tar</sub></code>. A learnable block embedding <code>e<sub>i</sub></code> is added to <code>q</code> to align the concept domain of i-th diffusion block.
          <p>
          <p>
            (b) We employ an anti-query <code>q<sub>a</sub></code> to capture irrelevant concepts <code>f<sup>a</sup><sub>cpt</sub></code> in <code>I<sub>ref</sub></code>, and constrain the desired concept <code>f<sup>tar</sup><sub>cpt</sub></code> in <code>I<sub>tar</sub></code> to be similar to <code>f<sub>cpt</sub></code> and orthogonal to <code>f<sup>a</sup><sub>cpt</sub></code> by Contrastive Orthogonal Disentangled (COD) Learning.
          </p>
        </div>
      </div>
    </div>
    <!--/ Method. -->
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- Datasets. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-2">Paired Concept Disentangled Datasets</h2>
        <img src="./static/images/dataset.png"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
        <div class="content has-text-justified">
          <p>
            We design three data construction pipelines for the three concepts of <code>content</code>, 
            <code>style</code>, and <code>composition</code>, each pipeline uses GPT-4o to obtain 
            reference prompts <span class="mathjax"><i>I</i><sub>ref</sub></span>, target prompts 
            <span class="mathjax"><i>I</i><sub>tar</sub></span>, and concept guidance <span class="mathjax"><i>T</i><sub>cg</sub></span>, 
            and use different models to generate corresponding reference images <span class="mathjax"><i>I</i><sub>ref</sub></span> 
            and target images <span class="mathjax"><i>I</i><sub>tar</sub></span>.
          <p>
        </div>
      </div>
    </div>
    <!--/ Datasets. -->
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- Main Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-2">Main Results</h2>
        <img src="./static/images/case.png"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
      </div>
    </div>
    <!--/ Main Results. -->
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- Comparisons. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-2">Comparisons with State-of-the-Arts</h2>
        <img src="./static/images/compare.png"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
        <div class="content has-text-justified">
          <p>
            Given a reference image with various concepts, our OmniPrism achieves superior disentangled generation performance. It not only avoids introducing irrelevant concepts but also ensures the highest concept and prompt fidelity and image quality.
          <p>
        </div>
        <img src="./static/images/table1.png"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
        <div class="content has-text-justified">
          <p>
            OmniPrism achieves the highest Mask CLIP-I, CLIP-T scores and image quality, and the best balance between Style Similarity and other metrics.
          <p>
        </div>
      </div>
    </div>
    <!--/ Comparisons. -->
  </div>
</section>

<section class="section hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- App. -->
      <h2 class="title is-2 has-text-centered">Applications</h2>
        <!-- combina. -->
        <div class="column is-full-width">
          <h2 class="title is-4 has-text-centered">Multi-Content Combinations</h2>
          <img src="./static/images/multi_obj.png"
            class="interpolation-image"
            alt="Interpolate start reference image."/>
          <div class="content has-text-justified">
            <p>
              We use latent masks to assign layouts to different concepts to prevent them from conflicting.
            <p>
          </div>
        </div>
        <!-- combina. -->
        <!-- blend -->
        <div class="column is-full-width">
          <h2 class="title is-4  has-text-centered">Concept Blending</h2>
          <img src="./static/images/blend.png"
            class="interpolation-image"
            alt="Interpolate start reference image."/>
        </div>
        <!-- blend -->
        <!-- controlnet -->
        <div class="column is-full-width">
          <h2 class="title is-4  has-text-centered">OmniPrism with ControlNet </h2>
          <img src="./static/images/ctrl_omni.png"
            class="interpolation-image"
            alt="Interpolate start reference image."/>
        </div>
        <!-- controlnet -->

      <!--/ App. -->
    </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- More Result. -->
      <h2 class="title is-2 has-text-centered">More Result</h2>
      <!-- base model. -->
      <div class="column is-full-width">
        <h2 class="title is-4 has-text-centered">Different Base model</h2>
        <img src="./static/images/diff_model.png"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
      </div>
      <!-- base model. -->
      <!-- obj. -->
      <div class="column is-full-width">
        <h2 class="title is-4 has-text-centered">Content Disentangle Results</h2>
        <img src="./static/images/more_obj.png"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
      </div>
      <!-- obj. -->
      <!-- style -->
      <div class="column is-full-width">
        <h2 class="title is-4 has-text-centered">Style Disentangle Results</h2>
        <img src="./static/images/more_style.png"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
      </div>
      <!-- style -->
      <!-- composition -->
      <div class="column is-full-width">
        <h2 class="title is-4 has-text-centered">Composition Disentangle Results</h2>
        <img src="./static/images/more_pose.png"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
      </div>
      <!-- composition -->
      <!--/ More Result. -->
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/Tale17" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
